{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmjSXWx6nYrRuaaUDbBD/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abasu0713/machine-learning-notebooks/blob/master/optimus%5Bbeta%5D-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Optimus - Trainer</h1>\n",
        "<p>This is an experimental Jupyter Notebook for training deep reinforcement learning models on stock market data pulled and saved as files following the instructions in this <a href=\"https://github.com/abasu0713/machine-learning-notebooks/blob/master/optimus%5Bbeta%5D-data.ipynb\">Notebook</a></p>"
      ],
      "metadata": {
        "id": "dAa3tyKqWk8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Training</h1>"
      ],
      "metadata": {
        "id": "yYhgIJ5fwoUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMSJ-49dw8rq",
        "outputId": "b97cdc85-f2d5-4c49-a2ef-3569342c99ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Requirement already satisfied: wrds in /usr/local/lib/python3.10/dist-packages (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from wrds) (1.5.3)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (from wrds) (2.9.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.10.1)\n",
            "Requirement already satisfied: sqlalchemy<2 in /usr/local/lib/python3.10/dist-packages (from wrds) (1.4.49)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2->wrds) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->wrds) (1.16.0)\n",
            "Requirement already satisfied: pyportfolioopt in /usr/local/lib/python3.10/dist-packages (1.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.5.3)\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.10.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyportfolioopt) (2023.3.post1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-dw5f84o1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-dw5f84o1\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit b781b80727bb8848e7addcfd6fa61e1ff6b06fa3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-q40uvkh2/elegantrl_e288a8352f064785830dd3ee6ed1ad3a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-q40uvkh2/elegantrl_e288a8352f064785830dd3ee6ed1ad3a\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 4c1893aeeb350e952bdcce916b13e6650a84cd68\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.1.60)\n",
            "Requirement already satisfied: exchange-calendars<5,>=4 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (4.5)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.9.0)\n",
            "Requirement already satisfied: pyfolio<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.9.2)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.5.5)\n",
            "Requirement already satisfied: ray[default,tune]<3,>=2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.2.2)\n",
            "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.5.4)\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.2.28)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.16)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.6.2)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
            "Requirement already satisfied: aiohttp==3.8.2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.8.2)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.1)\n",
            "Requirement already satisfied: multidict<6.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2023.7.22)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (41.0.3)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2023.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.49)\n",
            "Requirement already satisfied: thriftpy2>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.4.16)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.10.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.2)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.19.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.20.3)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.57.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.14)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.2)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.10.12)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.17.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.3.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.21.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (9.0.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.29.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.1+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.13.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.5.2)\n",
            "Requirement already satisfied: shimmy[atari]~=1.1.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.7)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.25.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.3.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.15.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.3)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.10/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (12.535.108)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.41.2)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.10/dist-packages (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (16.0.6)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.7)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (3.10.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.10.2)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.11.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.0.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ],
      "metadata": {
        "id": "MXVQ_1b41K96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train-stock-market-data.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure\n",
        "# it has the columns and index in the form that could be make into the environment.\n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ],
      "metadata": {
        "id": "qrPA19TBz_39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwf1THF51Yxz",
        "outputId": "fe218da0-a011-40f4-99d9-f58498ff82a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ],
      "metadata": {
        "id": "Ib-gH15b1lSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrlZxTSD1xBO",
        "outputId": "839bcf0f-d63a-47db-8f9f-0be8451a01cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ],
      "metadata": {
        "id": "HMX8QRX82TcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56zCsB8P2XhO",
        "outputId": "75390f24-059e-445d-fc9d-d932e2bfdee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Ku594p2lOk",
        "outputId": "69738c7d-fb54-4345-cb8e-d19fc047f6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 63          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.0101      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -18.5       |\n",
            "|    reward             | 0.046986707 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.04        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 79        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -52.6     |\n",
            "|    reward             | -1.586916 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.87      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -492      |\n",
            "|    reward             | 7.1933494 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 132       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 90       |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 47.7     |\n",
            "|    reward             | 4.753062 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 93         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 626        |\n",
            "|    reward             | -13.471534 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 287        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 95         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 186        |\n",
            "|    reward             | -0.1715455 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 21.7       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 96         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 4.77e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -75.5      |\n",
            "|    reward             | -3.2922494 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 97         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0.303      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 116        |\n",
            "|    reward             | -2.3530066 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 9.57       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 45         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 73.4       |\n",
            "|    reward             | -3.2809336 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.02       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 99        |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 10.9      |\n",
            "|    reward             | -6.007073 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.09      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 99       |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -756     |\n",
            "|    reward             | 7.677925 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 329      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 59         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0.000607   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -77.7      |\n",
            "|    reward             | 0.20513499 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.4        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 64        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -25.3     |\n",
            "|    reward             | -5.797168 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.97      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 69        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 109       |\n",
            "|    reward             | 0.8004914 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.61      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 74         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 94.7       |\n",
            "|    reward             | -0.5747191 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.23       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 78         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 91.7       |\n",
            "|    reward             | -3.5062914 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.23       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 101       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 83        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -307      |\n",
            "|    reward             | 1.7580183 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 68.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 101       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 88        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -14.7     |\n",
            "|    reward             | 1.3656312 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.652     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 102        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 92         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.021     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -16.4      |\n",
            "|    reward             | -0.6737915 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.2        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 102         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 97          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -8.59       |\n",
            "|    reward             | 0.031124644 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.735       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 102       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 102       |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.148     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 43.9      |\n",
            "|    reward             | 2.7099144 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.3       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 102        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 107        |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.0486     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -63.4      |\n",
            "|    reward             | -11.079048 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 18.4       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 102      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | -0.00313 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -1.9e+03 |\n",
            "|    reward             | 4.127899 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.56e+03 |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 102         |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 116         |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | 57.7        |\n",
            "|    reward             | 0.056121707 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 6.86        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 102        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 121        |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.00461    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -62.4      |\n",
            "|    reward             | 0.38291204 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.92       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 102         |\n",
            "|    iterations         | 2600        |\n",
            "|    time_elapsed       | 126         |\n",
            "|    total_timesteps    | 13000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2599        |\n",
            "|    policy_loss        | -63.5       |\n",
            "|    reward             | -0.18850757 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.45        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 103        |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 130        |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | -25        |\n",
            "|    reward             | -2.4866471 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.675      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 103      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 135      |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 416      |\n",
            "|    reward             | 5.259258 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 138      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 140       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.186    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -58.1     |\n",
            "|    reward             | 1.6210085 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 145       |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 65.5      |\n",
            "|    reward             | 1.1015614 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.09      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 103        |\n",
            "|    iterations         | 3100       |\n",
            "|    time_elapsed       | 149        |\n",
            "|    total_timesteps    | 15500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3099       |\n",
            "|    policy_loss        | 107        |\n",
            "|    reward             | 0.06036182 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.21       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 154       |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 11.7      |\n",
            "|    reward             | -2.590064 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.65      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 103        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 159        |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | -87.5      |\n",
            "|    reward             | -1.9183521 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 11         |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 103      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 163      |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 67       |\n",
            "|    reward             | 5.632623 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 34       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 103        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 168        |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -0.00642   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | 128        |\n",
            "|    reward             | 0.23283757 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 12.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 173       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -164      |\n",
            "|    reward             | 1.7804041 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 21.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 178       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 205       |\n",
            "|    reward             | 1.6350797 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 31.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 182       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 31.1      |\n",
            "|    reward             | 1.2797731 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.17      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 103       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 187       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -340      |\n",
            "|    reward             | 3.3164895 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 67.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 104      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 192      |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -246     |\n",
            "|    reward             | 3.365921 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 43.8     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 196        |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | -25.5      |\n",
            "|    reward             | 0.50703377 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.91       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 201        |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -173       |\n",
            "|    reward             | 0.07759519 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 20.8       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 104      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 206      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -163     |\n",
            "|    reward             | 5.413393 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 17.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 211       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 109       |\n",
            "|    reward             | 3.2279806 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 215       |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0.0313    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 144       |\n",
            "|    reward             | 2.1104946 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 220       |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 14.2      |\n",
            "|    reward             | -8.519498 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.01      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 225       |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0.000836  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -69.7     |\n",
            "|    reward             | 1.5193701 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.59      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 230        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -123       |\n",
            "|    reward             | -1.2181287 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 14.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 235       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -110      |\n",
            "|    reward             | 1.4441779 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 14.6      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 239         |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | 135         |\n",
            "|    reward             | -0.07244436 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 19.8        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 244        |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 798        |\n",
            "|    reward             | -0.7071867 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 376        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 249       |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -364      |\n",
            "|    reward             | 15.568361 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 198       |\n",
            "-------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6246313.86\n",
            "total_reward: 5246313.86\n",
            "total_cost: 8962.42\n",
            "total_trades: 47719\n",
            "Sharpe: 0.816\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 254       |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -0.0636   |\n",
            "|    reward             | 0.3333046 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.312     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 258       |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -130      |\n",
            "|    reward             | 0.4506015 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 263       |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -0.000318 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 200       |\n",
            "|    reward             | 4.8927717 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 27        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 268        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -157       |\n",
            "|    reward             | -1.6893584 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 16.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 273       |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -435      |\n",
            "|    reward             | -4.351599 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 129       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 277       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0.000391  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -30       |\n",
            "|    reward             | 1.1123973 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.45      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 104          |\n",
            "|    iterations         | 5900         |\n",
            "|    time_elapsed       | 282          |\n",
            "|    total_timesteps    | 29500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.8        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5899         |\n",
            "|    policy_loss        | 51.1         |\n",
            "|    reward             | -0.119555116 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 2.01         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 287        |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 65.6       |\n",
            "|    reward             | -0.8237876 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 3.29       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 292        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -1.54      |\n",
            "|    reward             | -2.1590443 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 296        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 15         |\n",
            "|    reward             | -1.0901645 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.37       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 104      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 301      |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 258      |\n",
            "|    reward             | 5.534793 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 76.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 306       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 5.19e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 94        |\n",
            "|    reward             | 1.4329511 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 5.74      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 310        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | 7.03       |\n",
            "|    reward             | -4.1020713 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 7.69       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 315         |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | -74.9       |\n",
            "|    reward             | -0.05231809 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 6.81        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6700       |\n",
            "|    time_elapsed       | 320        |\n",
            "|    total_timesteps    | 33500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0.011      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6699       |\n",
            "|    policy_loss        | -626       |\n",
            "|    reward             | -6.8995986 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 293        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 325       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -216      |\n",
            "|    reward             | 0.3951139 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 28.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 6900       |\n",
            "|    time_elapsed       | 329        |\n",
            "|    total_timesteps    | 34500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6899       |\n",
            "|    policy_loss        | -53.9      |\n",
            "|    reward             | 0.23920886 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 29.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 334        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0.0708     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 80.9       |\n",
            "|    reward             | 0.35854658 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 4.48       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 339       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 60.2      |\n",
            "|    reward             | 1.2095549 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.25      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 343        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -357       |\n",
            "|    reward             | 0.21574357 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 73.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 348       |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 129       |\n",
            "|    reward             | 1.7608211 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 353        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 131        |\n",
            "|    reward             | -7.1915336 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 21.9       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 7500        |\n",
            "|    time_elapsed       | 358         |\n",
            "|    total_timesteps    | 37500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7499        |\n",
            "|    policy_loss        | -39.5       |\n",
            "|    reward             | -0.56186384 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 2.59        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 362       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -97.8     |\n",
            "|    reward             | 2.7600179 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 7.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 104      |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 367      |\n",
            "|    total_timesteps    | 38500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | -11.5    |\n",
            "|    reward             | 0.972412 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 7800       |\n",
            "|    time_elapsed       | 372        |\n",
            "|    total_timesteps    | 39000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7799       |\n",
            "|    policy_loss        | 53.7       |\n",
            "|    reward             | 0.61646014 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.66       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 376       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 178       |\n",
            "|    reward             | 2.2965815 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 25.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 381       |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 57.6      |\n",
            "|    reward             | 0.7220056 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 4.4       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 104      |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 386      |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 92.5     |\n",
            "|    reward             | 5.385799 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 8.73     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 391       |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | -0.26     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 83.5      |\n",
            "|    reward             | 0.5018017 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 5.77      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 395       |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | -94.5     |\n",
            "|    reward             | -1.865133 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 4.94      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 8400       |\n",
            "|    time_elapsed       | 400        |\n",
            "|    total_timesteps    | 42000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8399       |\n",
            "|    policy_loss        | -84.3      |\n",
            "|    reward             | -3.1646488 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 5.39       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 8500        |\n",
            "|    time_elapsed       | 405         |\n",
            "|    total_timesteps    | 42500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.3       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8499        |\n",
            "|    policy_loss        | -116        |\n",
            "|    reward             | -0.67722374 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 9.53        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 8600       |\n",
            "|    time_elapsed       | 410        |\n",
            "|    total_timesteps    | 43000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | -0.0329    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8599       |\n",
            "|    policy_loss        | 167        |\n",
            "|    reward             | -7.8288817 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 37.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 414       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 17.6      |\n",
            "|    reward             | 1.6171181 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.05      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 419       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 6.68      |\n",
            "|    reward             | 0.2980302 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 3.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 8900       |\n",
            "|    time_elapsed       | 424        |\n",
            "|    total_timesteps    | 44500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8899       |\n",
            "|    policy_loss        | 0.813      |\n",
            "|    reward             | -1.5759765 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.455      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 429         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 3.45        |\n",
            "|    reward             | -0.48438716 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.306       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 433       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 4.86      |\n",
            "|    reward             | 1.7437936 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.607     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 438       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | -6.86     |\n",
            "|    reward             | 6.7979784 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 10.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 9300       |\n",
            "|    time_elapsed       | 443        |\n",
            "|    total_timesteps    | 46500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9299       |\n",
            "|    policy_loss        | -153       |\n",
            "|    reward             | 0.18884683 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 11.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 9400       |\n",
            "|    time_elapsed       | 448        |\n",
            "|    total_timesteps    | 47000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | -0.3       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9399       |\n",
            "|    policy_loss        | 166        |\n",
            "|    reward             | -0.6371377 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 19         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 104         |\n",
            "|    iterations         | 9500        |\n",
            "|    time_elapsed       | 452         |\n",
            "|    total_timesteps    | 47500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9499        |\n",
            "|    policy_loss        | 178         |\n",
            "|    reward             | -0.14966334 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 18.8        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 104        |\n",
            "|    iterations         | 9600       |\n",
            "|    time_elapsed       | 457        |\n",
            "|    total_timesteps    | 48000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9599       |\n",
            "|    policy_loss        | -591       |\n",
            "|    reward             | -1.0033163 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 192        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 462       |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 140       |\n",
            "|    reward             | -1.158149 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 13.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 467       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | -15       |\n",
            "|    reward             | 5.6256747 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 28.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 471       |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 59.2      |\n",
            "|    reward             | 0.6734147 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 2.59      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 104       |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 476       |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | -22.7     |\n",
            "|    reward             | -1.603473 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.04      |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ],
      "metadata": {
        "id": "ZAomMZ5r4wYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PHzR7bP46KJ",
        "outputId": "5438b9e4-44f4-4374-cb61-b18e351abb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JWKmnrp4-lo",
        "outputId": "8e6d58bb-97fd-4bbe-8b9a-8ec43a43f722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5077013.26\n",
            "total_reward: 4077013.26\n",
            "total_cost: 4479.94\n",
            "total_trades: 37153\n",
            "Sharpe: 0.837\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 82       |\n",
            "|    time_elapsed    | 139      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.3     |\n",
            "|    critic_loss     | 91.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8679     |\n",
            "|    reward          | 4.443804 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 78       |\n",
            "|    time_elapsed    | 294      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 4.44     |\n",
            "|    critic_loss     | 11.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 20251    |\n",
            "|    reward          | 4.443804 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6000662.38\n",
            "total_reward: 5000662.38\n",
            "total_cost: 999.00\n",
            "total_trades: 37596\n",
            "Sharpe: 0.885\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 449      |\n",
            "|    total_timesteps | 34716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -4.3     |\n",
            "|    critic_loss     | 4.59     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31823    |\n",
            "|    reward          | 4.443804 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 76       |\n",
            "|    time_elapsed    | 604      |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -9.16    |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43395    |\n",
            "|    reward          | 4.443804 |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"agent_ddpg\") if if_using_ddpg else None"
      ],
      "metadata": {
        "id": "257ttBMn60cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653sjPhP8g12",
        "outputId": "48fd0306-4297-49a4-95b9-2ef59c1a25e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None\n",
        "\n",
        "trained_ppo.save(TRAINED_MODEL_DIR + \"agent_ppo\") if if_using_ppo else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_XTEZrI8l7t",
        "outputId": "bce5a43c-ec54-4df7-9d6e-ade2e4183ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 116       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 17        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.1890339 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 113         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 36          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016410885 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0013     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.02        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0288     |\n",
            "|    reward               | 0.7251011   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3938733.56\n",
            "total_reward: 2938733.56\n",
            "total_cost: 343295.33\n",
            "total_trades: 81067\n",
            "Sharpe: 0.793\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 112        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 54         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01707068 |\n",
            "|    clip_fraction        | 0.176      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.3      |\n",
            "|    explained_variance   | 0.00386    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 20.9       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0204    |\n",
            "|    reward               | -1.6243037 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 53.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 112         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 73          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015689809 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00922     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.1        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    reward               | 2.2977273   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 84.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 112         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 91          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017659087 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.0017     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.83        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0195     |\n",
            "|    reward               | 2.716629    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 20.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 109         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017461706 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.0032     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.6        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.021      |\n",
            "|    reward               | 1.927716    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 58.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 128         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016820906 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.00732    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    reward               | 1.9853871   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 62.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 146         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019355081 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.00346     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.7        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    reward               | -0.08859944 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 43          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 164         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019983295 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.00626    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 73          |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    reward               | 0.4473209   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 72.1        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 183        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02089993 |\n",
            "|    clip_fraction        | 0.275      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.6      |\n",
            "|    explained_variance   | -0.00573   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 51.7       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0156    |\n",
            "|    reward               | 0.54325134 |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 66.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 202         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021710806 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.0318     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.7        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    reward               | 1.6466471   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 68.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 220         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020178083 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.0175     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.35        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0171     |\n",
            "|    reward               | 0.095857814 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 17.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 238         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021324355 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0213      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.5        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    reward               | 0.031733006 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 64          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 257        |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01796798 |\n",
            "|    clip_fraction        | 0.19       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.9      |\n",
            "|    explained_variance   | 0.00669    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 36.7       |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0161    |\n",
            "|    reward               | 1.6160063  |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 109        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 275         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018528488 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.0783     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.72        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    reward               | 3.8785136   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 19.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 294         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017903205 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.000978    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.9        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0224     |\n",
            "|    reward               | -0.21053165 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 43.9        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3484965.64\n",
            "total_reward: 2484965.64\n",
            "total_cost: 306369.10\n",
            "total_trades: 78099\n",
            "Sharpe: 0.764\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 312         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021161344 |\n",
            "|    clip_fraction        | 0.189       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | -0.00472    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 56.8        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    reward               | 0.3302072   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 70.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 331         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027062654 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.00789     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13          |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.01       |\n",
            "|    reward               | -0.14741008 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 45.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 349         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023556668 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | -0.0144     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.11        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0246     |\n",
            "|    reward               | 0.19218874  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 367         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018249784 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | -0.00353    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.2        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0195     |\n",
            "|    reward               | -0.30429074 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 54.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 386         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023325674 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | -0.00099    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.7        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | -12.752791  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 67          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 404         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025875755 |\n",
            "|    clip_fraction        | 0.27        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0132      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.3        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0152     |\n",
            "|    reward               | 2.8338778   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 34.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 423         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032998964 |\n",
            "|    clip_fraction        | 0.277       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.00964     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.2        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0196     |\n",
            "|    reward               | 0.30111063  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 74.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 441         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020189192 |\n",
            "|    clip_fraction        | 0.236       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | -0.00445    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.2        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    reward               | 1.7222669   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 62          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 459         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018327268 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | -0.0141     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.5        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | -0.5852801  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 55.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 478         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022377547 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.00323     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.63        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    reward               | 1.6030345   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 18.9        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 27         |\n",
            "|    time_elapsed         | 496        |\n",
            "|    total_timesteps      | 55296      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01758827 |\n",
            "|    clip_fraction        | 0.146      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.4      |\n",
            "|    explained_variance   | -0.00436   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 25.5       |\n",
            "|    n_updates            | 260        |\n",
            "|    policy_gradient_loss | -0.0149    |\n",
            "|    reward               | -1.1813844 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 62.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 515         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022924442 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | -0.0199     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26          |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0221     |\n",
            "|    reward               | -0.07158759 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 53.1        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 533        |\n",
            "|    total_timesteps      | 59392      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02612482 |\n",
            "|    clip_fraction        | 0.253      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.5      |\n",
            "|    explained_variance   | -0.0741    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 7.03       |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | -0.0151    |\n",
            "|    reward               | 0.21396667 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 15.5       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 552        |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02110828 |\n",
            "|    clip_fraction        | 0.192      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.00201    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 35         |\n",
            "|    n_updates            | 290        |\n",
            "|    policy_gradient_loss | -0.016     |\n",
            "|    reward               | 0.6382566  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 31.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 570         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018308833 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.0236      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.8        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0196     |\n",
            "|    reward               | 0.17389564  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 33.7        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2537944.85\n",
            "total_reward: 1537944.85\n",
            "total_cost: 289025.90\n",
            "total_trades: 76409\n",
            "Sharpe: 0.534\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 589         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025198251 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.035       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10          |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 0.20675957  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 20.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 607         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019581549 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.00898     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.8        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    reward               | -0.8071716  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 34.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 626         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025859773 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.00869     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.9        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | 0.6957159   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 42.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 644         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019100234 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.0515      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.3        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    reward               | -0.62644106 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 45.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 111        |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 663        |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03752308 |\n",
            "|    clip_fraction        | 0.347      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.8      |\n",
            "|    explained_variance   | 0.00295    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 10.2       |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.00851   |\n",
            "|    reward               | -2.9662182 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 22.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 111         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 682         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033390142 |\n",
            "|    clip_fraction        | 0.302       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | -0.00361    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34.8        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | -0.49019533 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 50.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 110        |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 701        |\n",
            "|    total_timesteps      | 77824      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02778861 |\n",
            "|    clip_fraction        | 0.256      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.8      |\n",
            "|    explained_variance   | 0.0163     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 20.5       |\n",
            "|    n_updates            | 370        |\n",
            "|    policy_gradient_loss | -0.0188    |\n",
            "|    reward               | -4.2604637 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 40.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 720         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025223088 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.00625     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.42        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0125     |\n",
            "|    reward               | -1.8430063  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 17.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 739         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035291612 |\n",
            "|    clip_fraction        | 0.303       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.0333      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.7        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | -0.14213315 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 34.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 110        |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 757        |\n",
            "|    total_timesteps      | 83968      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02702669 |\n",
            "|    clip_fraction        | 0.249      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43        |\n",
            "|    explained_variance   | -0.017     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 25         |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | -0.0122    |\n",
            "|    reward               | 0.33719957 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 55.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 776         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038861506 |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | -0.0198     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.3        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00894    |\n",
            "|    reward               | 0.29323974  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 45.9        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 110       |\n",
            "|    iterations           | 43        |\n",
            "|    time_elapsed         | 795       |\n",
            "|    total_timesteps      | 88064     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0401594 |\n",
            "|    clip_fraction        | 0.335     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -43.1     |\n",
            "|    explained_variance   | -0.0261   |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 8.19      |\n",
            "|    n_updates            | 420       |\n",
            "|    policy_gradient_loss | -0.0111   |\n",
            "|    reward               | -1.064866 |\n",
            "|    std                  | 1.07      |\n",
            "|    value_loss           | 16.3      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 814         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025380988 |\n",
            "|    clip_fraction        | 0.315       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0229      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0114     |\n",
            "|    reward               | 0.21038677  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 50.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 833         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036811408 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.00378     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.38        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00531    |\n",
            "|    reward               | -1.3891655  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 43.7        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4634094.82\n",
            "total_reward: 3634094.82\n",
            "total_cost: 308007.46\n",
            "total_trades: 77039\n",
            "Sharpe: 0.906\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 852         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030479658 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.0358      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.8        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    reward               | -6.3344703  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 20.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 871         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036533378 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.0297      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.9        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0148     |\n",
            "|    reward               | 1.4132138   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 65.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 890         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029833222 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.0288      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 59.3        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00783    |\n",
            "|    reward               | 13.830982   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 102         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 908         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021119948 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.0628      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.5        |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | -1.539597   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 65          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 927         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.0481828   |\n",
            "|    clip_fraction        | 0.336       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | -0.0101     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.3        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.00921    |\n",
            "|    reward               | -0.08598764 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 24.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 946         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03134344  |\n",
            "|    clip_fraction        | 0.272       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | -0.0216     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.8        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    reward               | -0.28646347 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 84.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 965         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024671867 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.0223      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.6        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    reward               | -3.4683194  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 73.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 984         |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030580029 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.2        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    reward               | 1.3964158   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 28.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 1003        |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026961211 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.0692      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.2        |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    reward               | -0.17792428 |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 51.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 110        |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 1022       |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03564421 |\n",
            "|    clip_fraction        | 0.331      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.6      |\n",
            "|    explained_variance   | -0.00979   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 70.4       |\n",
            "|    n_updates            | 540        |\n",
            "|    policy_gradient_loss | -0.00317   |\n",
            "|    reward               | 5.959024   |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 98.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1041        |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030251995 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0112      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.7        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.0115     |\n",
            "|    reward               | 2.3785563   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 62.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1060        |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031367626 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0572      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.5        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 1.3469526   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 85.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 1079        |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040255606 |\n",
            "|    clip_fraction        | 0.366       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.0109      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.3        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00812    |\n",
            "|    reward               | 0.9164384   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 67.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 1097        |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033853576 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.0111      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 65.4        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00476    |\n",
            "|    reward               | -1.2986504  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 77.6        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3962791.06\n",
            "total_reward: 2962791.06\n",
            "total_cost: 250693.71\n",
            "total_trades: 72689\n",
            "Sharpe: 0.739\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 1116        |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.052240103 |\n",
            "|    clip_fraction        | 0.361       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.0893      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.48        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    reward               | 0.20230773  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1135        |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024421157 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.0589      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.4        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    reward               | 0.71669847  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 72.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1154        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035605963 |\n",
            "|    clip_fraction        | 0.278       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.0597      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.9        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00993    |\n",
            "|    reward               | 5.7773      |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 104         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 63         |\n",
            "|    time_elapsed         | 1173       |\n",
            "|    total_timesteps      | 129024     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03326536 |\n",
            "|    clip_fraction        | 0.324      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44        |\n",
            "|    explained_variance   | 0.0736     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 17.1       |\n",
            "|    n_updates            | 620        |\n",
            "|    policy_gradient_loss | -0.00191   |\n",
            "|    reward               | 1.8666149  |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 36.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1192        |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055605352 |\n",
            "|    clip_fraction        | 0.389       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.0311      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.9        |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | 0.00583     |\n",
            "|    reward               | -0.37063178 |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 75.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1211        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025373574 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 91.9        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.00379    |\n",
            "|    reward               | -1.2393421  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 145         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 66         |\n",
            "|    time_elapsed         | 1229       |\n",
            "|    total_timesteps      | 135168     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03033122 |\n",
            "|    clip_fraction        | 0.298      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.2      |\n",
            "|    explained_variance   | 0.0403     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 55.9       |\n",
            "|    n_updates            | 650        |\n",
            "|    policy_gradient_loss | -0.00183   |\n",
            "|    reward               | 2.6582022  |\n",
            "|    std                  | 1.11       |\n",
            "|    value_loss           | 83.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 1248        |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030005317 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.37        |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.00568    |\n",
            "|    reward               | -1.6967685  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 23.2        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 109          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 1267         |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.029948318  |\n",
            "|    clip_fraction        | 0.208        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.2        |\n",
            "|    explained_variance   | 0.0965       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 58.1         |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -1.57e-05    |\n",
            "|    reward               | -0.012626032 |\n",
            "|    std                  | 1.11         |\n",
            "|    value_loss           | 117          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 69         |\n",
            "|    time_elapsed         | 1286       |\n",
            "|    total_timesteps      | 141312     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04541303 |\n",
            "|    clip_fraction        | 0.313      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.2      |\n",
            "|    explained_variance   | 0.0375     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 19.7       |\n",
            "|    n_updates            | 680        |\n",
            "|    policy_gradient_loss | -0.00718   |\n",
            "|    reward               | 2.4170299  |\n",
            "|    std                  | 1.11       |\n",
            "|    value_loss           | 51.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1305        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036842175 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.451       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.12        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    reward               | 0.28109047  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 17.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 109          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 1324         |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0152969835 |\n",
            "|    clip_fraction        | 0.159        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.3        |\n",
            "|    explained_variance   | 0.0984       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 14.5         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.013       |\n",
            "|    reward               | 0.37041116   |\n",
            "|    std                  | 1.12         |\n",
            "|    value_loss           | 60.4         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 72         |\n",
            "|    time_elapsed         | 1343       |\n",
            "|    total_timesteps      | 147456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03649431 |\n",
            "|    clip_fraction        | 0.275      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.3      |\n",
            "|    explained_variance   | 0.0788     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 50.8       |\n",
            "|    n_updates            | 710        |\n",
            "|    policy_gradient_loss | 0.000512   |\n",
            "|    reward               | -14.680717 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 95.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 1362        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012106117 |\n",
            "|    clip_fraction        | 0.0967      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0748      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.5        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00541    |\n",
            "|    reward               | -3.2971337  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 69          |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6139458.81\n",
            "total_reward: 5139458.81\n",
            "total_cost: 218002.05\n",
            "total_trades: 71603\n",
            "Sharpe: 0.881\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1381        |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044529665 |\n",
            "|    clip_fraction        | 0.33        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.00294     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.2        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.00822    |\n",
            "|    reward               | -1.9114555  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 46.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1400        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018992923 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0581      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 144         |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00146    |\n",
            "|    reward               | 1.3974826   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 209         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 76         |\n",
            "|    time_elapsed         | 1419       |\n",
            "|    total_timesteps      | 155648     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01950429 |\n",
            "|    clip_fraction        | 0.226      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.4      |\n",
            "|    explained_variance   | 0.0699     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 35.7       |\n",
            "|    n_updates            | 750        |\n",
            "|    policy_gradient_loss | 0.00247    |\n",
            "|    reward               | -2.7664907 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 97.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1438        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017226614 |\n",
            "|    clip_fraction        | 0.189       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0415      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.8        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00529    |\n",
            "|    reward               | 2.1156235   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 29.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 78         |\n",
            "|    time_elapsed         | 1457       |\n",
            "|    total_timesteps      | 159744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02652094 |\n",
            "|    clip_fraction        | 0.27       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.5      |\n",
            "|    explained_variance   | 0.0583     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 24.4       |\n",
            "|    n_updates            | 770        |\n",
            "|    policy_gradient_loss | -0.00497   |\n",
            "|    reward               | 1.7242311  |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 94.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 1475        |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011816311 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.3        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00769    |\n",
            "|    reward               | 1.4035985   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 135         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1494        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014950579 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.0903      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00554    |\n",
            "|    reward               | -0.1779322  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 47.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1513        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029137172 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.103       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.8        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00626    |\n",
            "|    reward               | -0.35035938 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 54.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1532        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023275696 |\n",
            "|    clip_fraction        | 0.279       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.0898      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 40.8        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | 3.95e-05    |\n",
            "|    reward               | 0.067347765 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 80.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 83         |\n",
            "|    time_elapsed         | 1551       |\n",
            "|    total_timesteps      | 169984     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01614295 |\n",
            "|    clip_fraction        | 0.28       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.044      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 44.2       |\n",
            "|    n_updates            | 820        |\n",
            "|    policy_gradient_loss | 0.00504    |\n",
            "|    reward               | 0.49777475 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 122        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 1570        |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.04719418  |\n",
            "|    clip_fraction        | 0.268       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.156       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.6        |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.00558    |\n",
            "|    reward               | -0.10268734 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 23.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1589        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02274226  |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.0978      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 73.2        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | -0.00343    |\n",
            "|    reward               | -0.06288893 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1607        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011833524 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.0542      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 69.9        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00795    |\n",
            "|    reward               | -0.6468127  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 105         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1626        |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031000745 |\n",
            "|    clip_fraction        | 0.273       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.06        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.8        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | 0.00253     |\n",
            "|    reward               | 2.1516912   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 74.3        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3836392.31\n",
            "total_reward: 2836392.31\n",
            "total_cost: 240864.01\n",
            "total_trades: 72849\n",
            "Sharpe: 0.681\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1645        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030171195 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.0775      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 55.8        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00529    |\n",
            "|    reward               | -3.2746792  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 75          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 109          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1664         |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.041096628  |\n",
            "|    clip_fraction        | 0.227        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.9        |\n",
            "|    explained_variance   | 0.0662       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 63.2         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.000696     |\n",
            "|    reward               | -0.060181726 |\n",
            "|    std                  | 1.14         |\n",
            "|    value_loss           | 104          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 1683        |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015631724 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0646      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38          |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | -0.0045     |\n",
            "|    reward               | 0.68066275  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 85.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 109          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1702         |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.022670863  |\n",
            "|    clip_fraction        | 0.213        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.9        |\n",
            "|    explained_variance   | -0.018       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 7.36         |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    reward               | -0.054001026 |\n",
            "|    std                  | 1.14         |\n",
            "|    value_loss           | 18.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1721        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021885138 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.013       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.4        |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.00303    |\n",
            "|    reward               | -1.2548335  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 90.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 1739        |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021594232 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0416      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 79.6        |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | -0.00439    |\n",
            "|    reward               | -1.9409965  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 105         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1758        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014421209 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0443      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24          |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00864    |\n",
            "|    reward               | -0.90384984 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 45.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1777        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055496857 |\n",
            "|    clip_fraction        | 0.285       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0493      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 41.7        |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | 0.00207     |\n",
            "|    reward               | -1.4401351  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 68.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 96         |\n",
            "|    time_elapsed         | 1796       |\n",
            "|    total_timesteps      | 196608     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03889489 |\n",
            "|    clip_fraction        | 0.316      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.1      |\n",
            "|    explained_variance   | 0.00711    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 49.1       |\n",
            "|    n_updates            | 950        |\n",
            "|    policy_gradient_loss | 0.00604    |\n",
            "|    reward               | -2.7482922 |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 114        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 97         |\n",
            "|    time_elapsed         | 1815       |\n",
            "|    total_timesteps      | 198656     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02854028 |\n",
            "|    clip_fraction        | 0.268      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.2      |\n",
            "|    explained_variance   | 0.0675     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 41.2       |\n",
            "|    n_updates            | 960        |\n",
            "|    policy_gradient_loss | 0.00713    |\n",
            "|    reward               | 0.7378812  |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 96.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1834        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019938767 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.156       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.3        |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | -0.007      |\n",
            "|    reward               | -0.43447164 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 49.6        |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNQeEjqQEV4L",
        "outputId": "4991cbd6-2cd6-41b0-f8dc-2b6a024652cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/td3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None\n",
        "\n",
        "trained_td3.save(TRAINED_MODEL_DIR + \"agent_td3\") if if_using_td3 else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQI7KiP5EZ58",
        "outputId": "2a11e8ec-c2e9-43cb-a5da-9b22deafab7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 83       |\n",
            "|    time_elapsed    | 138      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 146      |\n",
            "|    critic_loss     | 1.1e+04  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8679     |\n",
            "|    reward          | 8.609434 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 79       |\n",
            "|    time_elapsed    | 290      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 63.6     |\n",
            "|    critic_loss     | 3.37e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 20251    |\n",
            "|    reward          | 8.609434 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5599582.72\n",
            "total_reward: 4599582.72\n",
            "total_cost: 1298.51\n",
            "total_trades: 31969\n",
            "Sharpe: 0.921\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 78       |\n",
            "|    time_elapsed    | 443      |\n",
            "|    total_timesteps | 34716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.6     |\n",
            "|    critic_loss     | 212      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31823    |\n",
            "|    reward          | 8.609434 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 595      |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 32.1     |\n",
            "|    critic_loss     | 44.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43395    |\n",
            "|    reward          | 8.609434 |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9U8fPCeNnE4",
        "outputId": "4b3703f9-f90c-4c67-ef17-82a119a3c52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cuda device\n",
            "Logging to results/sac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None\n",
        "\n",
        "trained_sac.save(TRAINED_MODEL_DIR + \"agent_sac\") if if_using_sac else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwJVPX78Nr9q",
        "outputId": "8fa4d1e2-4a6b-418c-b52c-697fbb549fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 53       |\n",
            "|    time_elapsed    | 216      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.54e+03 |\n",
            "|    critic_loss     | 270      |\n",
            "|    ent_coef        | 0.28     |\n",
            "|    ent_coef_loss   | 55.9     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 11471    |\n",
            "|    reward          | 5.494194 |\n",
            "---------------------------------\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 8          |\n",
            "|    fps             | 53         |\n",
            "|    time_elapsed    | 434        |\n",
            "|    total_timesteps | 23144      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 608        |\n",
            "|    critic_loss     | 80.2       |\n",
            "|    ent_coef        | 0.104      |\n",
            "|    ent_coef_loss   | -104       |\n",
            "|    learning_rate   | 0.0001     |\n",
            "|    n_updates       | 23043      |\n",
            "|    reward          | -5.0802135 |\n",
            "-----------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7592832.92\n",
            "total_reward: 6592832.92\n",
            "total_cost: 12704.12\n",
            "total_trades: 49879\n",
            "Sharpe: 0.920\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 652       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 233       |\n",
            "|    critic_loss     | 46.8      |\n",
            "|    ent_coef        | 0.0331    |\n",
            "|    ent_coef_loss   | -137      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | -4.610795 |\n",
            "----------------------------------\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 16         |\n",
            "|    fps             | 53         |\n",
            "|    time_elapsed    | 871        |\n",
            "|    total_timesteps | 46288      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 97.1       |\n",
            "|    critic_loss     | 20.3       |\n",
            "|    ent_coef        | 0.0107     |\n",
            "|    ent_coef_loss   | -137       |\n",
            "|    learning_rate   | 0.0001     |\n",
            "|    n_updates       | 46187      |\n",
            "|    reward          | -3.7745485 |\n",
            "-----------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5950048.09\n",
            "total_reward: 4950048.09\n",
            "total_cost: 2307.93\n",
            "total_trades: 49276\n",
            "Sharpe: 0.910\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 1089      |\n",
            "|    total_timesteps | 57860     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 54.7      |\n",
            "|    critic_loss     | 13.1      |\n",
            "|    ent_coef        | 0.00368   |\n",
            "|    ent_coef_loss   | -63       |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 57759     |\n",
            "|    reward          | 1.7876546 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 1307      |\n",
            "|    total_timesteps | 69432     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 39.1      |\n",
            "|    critic_loss     | 10.9      |\n",
            "|    ent_coef        | 0.00227   |\n",
            "|    ent_coef_loss   | 2.39      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 69331     |\n",
            "|    reward          | 2.3939617 |\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"/content/trained_modelsagent_ddpg.zip\")\n",
        "#files.download(\"/content/trained_modelsagent_ppo.zip\")\n",
        "#files.download(\"/content/trained_modelsagent_sac.zip\")\n",
        "#files.download(\"/content/trained_modelsagent_td3.zip\")\n",
        "files.download(\"/content/trained_models/agent_a2c.zip\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GGr_ChHmXC-g",
        "outputId": "ab4fef3d-acbc-4417-d06e-9b6635885495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b5b515e-7dbf-445e-8bc9-8cf7b3fe35d0\", \"agent_a2c.zip\", 411612)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}